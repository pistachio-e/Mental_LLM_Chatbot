import os
import pandas as pd
import numpy as np
import faiss
import pickle
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi

# === Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ ===
file_path = r"F:\code_payanname\dataset\no_llm_dataset.xlsx"
index_path = "faiss_index_combined.index"
meta_path = "metadata.pkl"

# === Ù…Ø¯Ù„ embedding ===
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# === Ø³Ø§Ø®Øª ÛŒØ§ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ ===
def build_index(alpha=0.3, beta=0.7, force_rebuild=False):
    global index, df
    if force_rebuild or not (os.path.exists(index_path) and os.path.exists(meta_path)):
        print(f"ğŸ”„ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ alpha={alpha}, beta={beta}...")
        df = pd.read_excel(file_path)
        df = df.drop_duplicates(subset=["Context", "Response"]).reset_index(drop=True)
        q_embeddings = embed_model.encode(df["Context"].tolist(), convert_to_numpy=True, normalize_embeddings=True).astype("float32")
        r_embeddings = embed_model.encode(df["Response"].tolist(), convert_to_numpy=True, normalize_embeddings=True).astype("float32")
        combined_embeddings = alpha * q_embeddings + beta * r_embeddings
        dim = combined_embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(combined_embeddings)
        faiss.write_index(index, index_path)
        with open(meta_path, "wb") as f:
            pickle.dump(df, f)
        print("âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ùˆ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÛŒÚ©ØªØ§ Ø§Ø³Øª.")
    else:
        print("ğŸ“‚ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„...")
        index = faiss.read_index(index_path)
        with open(meta_path, "rb") as f:
            df = pickle.load(f)
        print("âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ùˆ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯.")

# === ØªØ§Ø¨Ø¹ Ø¬Ø³ØªØ¬ÙˆÛŒ Hybrid RAG Ø¨Ø§ ÙÛŒÙ„ØªØ± Ùˆ BM25 Ø¨Ø¹Ø¯ FAISS ===
def search_hybrid_rag_final(query, top_k=1, candidate_pool=5, alpha_faiss=0.6, beta_q=0.2, gamma_r=0.2):
    # embedding query
    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype("float32")
    
    # FAISS search Ø¨Ø±Ø§ÛŒ candidate pool Ø¨Ø²Ø±Ú¯â€ŒØªØ±
    D, I = index.search(q_emb, k=min(candidate_pool*2, len(df)))

    # Ø¬Ù…Ø¹ Ø¢ÙˆØ±ÛŒ candidates Ùˆ Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
    seen_texts = set()
    candidates = []
    for i, faiss_score in zip(I[0], D[0]):
        context = df.iloc[i]["Context"].strip()
        response = df.iloc[i]["Response"].strip()
        key = (context, response)
        if key in seen_texts:
            continue
        seen_texts.add(key)
        candidates.append({
            "index": i,
            "Context": context,
            "Response": response,
            "FAISS_Score": faiss_score
        })
        if len(candidates) >= candidate_pool:
            break

    # BM25 Ø±ÙˆÛŒ candidates
    tokenized_query = query.split()
    tokenized_context = [c["Context"].split() for c in candidates]
    tokenized_response = [c["Response"].split() for c in candidates]
    bm25_context = BM25Okapi(tokenized_context)
    bm25_response = BM25Okapi(tokenized_response)
    bm25_scores_context = bm25_context.get_scores(tokenized_query)
    bm25_scores_response = bm25_response.get_scores(tokenized_query)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ final score
    for idx, c in enumerate(candidates):
        c["BM25_Context_Score"] = bm25_scores_context[idx]
        c["BM25_Response_Score"] = bm25_scores_response[idx]
        c["Score"] = alpha_faiss * c["FAISS_Score"] + beta_q * c["BM25_Context_Score"] + gamma_r * c["BM25_Response_Score"]

    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ final score Ùˆ top_k
    results = sorted(candidates, key=lambda x: x["Score"], reverse=True)[:top_k]
    return results

# === Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ ===
if __name__ == "__main__":
    # Ø³Ø§Ø®Øª ÛŒØ§ Ù„ÙˆØ¯ Ø§ÛŒÙ†Ø¯Ú©Ø³
    build_index(alpha=0.3, beta=0.7, force_rebuild=False)

    # Ù¾Ø±Ø³Ø´ Ú©Ø§Ø±Ø¨Ø±
    user_query = "hello, i have trauma can you help me ?"

    # Ø¬Ø³ØªØ¬ÙˆÛŒ Hybrid RAG Ùˆ Ú¯Ø±ÙØªÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø³Ø®
    results = search_hybrid_rag_final(user_query, top_k=1, candidate_pool=5, alpha_faiss=0.5, beta_q=0.2, gamma_r=0.3)
    best_response = results[0]["Response"]

    # Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø±Ø§ÛŒ LLM
    prompt = f"""### Instruction:
You are a helpful therapist.

Here is a relevant past response:
"{best_response}"

Now, a new user says:
"{user_query}"

Respond with empathy and practical advice.

### Response:
"""

    print("\nğŸ“ Ù¾Ø±Ø§Ù…Ù¾Øª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡:")
    print(prompt)

    # Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
    print("\nğŸ” Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Hybrid RAG:")
    r = results[0]
    print(f"Q: {r['Context']}")
    print(f"A: {r['Response']}")
    print(f"FAISS Score: {r['FAISS_Score']:.4f}")
    print(f"BM25 Context Score: {r['BM25_Context_Score']:.4f}")
    print(f"BM25 Response Score: {r['BM25_Response_Score']:.4f}")
    print(f"Final Score: {r['Score']:.4f}")
    print("-" * 50)
